{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMxq490q/1NhlxnsEtZ1JJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**General Linear Model**"],"metadata":{"id":"nlcwkPgO_ANu"}},{"cell_type":"markdown","source":["1. What is the purpose of the General Linear Model (GLM)?"],"metadata":{"id":"Rffha5UOolIv"}},{"cell_type":"markdown","source":["~ General Linear Models are a class of regression models that can be used to model a wide range of relationships between a response variable and one or more predictor variables. Unlike traditional linear regression models, which assume a linear relationship between the response and predictor variables, GLMs allow for more flexible, non-linear relationships by using a different underlying statistical distribution."],"metadata":{"id":"1j2kWG1RpKf4"}},{"cell_type":"markdown","source":["2. What are the key assumptions of the General Linear Model?"],"metadata":{"id":"5OAbIYVio1BE"}},{"cell_type":"markdown","source":["~ Linearity, constant variance, normality, independence, multicollinearity, gaussian distribution are key assumptions of the General Linear Model."],"metadata":{"id":"9-BjPI91o1NK"}},{"cell_type":"markdown","source":["3. How do you interpret the coefficients in a GLM?"],"metadata":{"id":"bpz6oUauo1YT"}},{"cell_type":"markdown","source":["~ A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."],"metadata":{"id":"ZacpYdGlo1jA"}},{"cell_type":"markdown","source":["4. What is the difference between a univariate and multivariate GLM?"],"metadata":{"id":"OIBVabEZo1uU"}},{"cell_type":"markdown","source":["~ In Univariate GLM there is only one independent variable and one dependent variable whereas in in Multivariate GLM there are more than one independent variables and one dependent variable."],"metadata":{"id":"uVjy1Z2-o15E"}},{"cell_type":"markdown","source":["5. Explain the concept of interaction effects in a GLM."],"metadata":{"id":"4gw6K_fgo2OB"}},{"cell_type":"markdown","source":["~ Interaction is the  effect of one variable depending on the value of the other variable with which it interacts. If there isn't an interaction, then the value of the other variable doesn't matter. This is easiest to understand in the case of linear regression."],"metadata":{"id":"Sl9VRYTEo2ZJ"}},{"cell_type":"markdown","source":["6. How do you handle categorical predictors in a GLM?"],"metadata":{"id":"M7K2AbC9o2j9"}},{"cell_type":"markdown","source":["~ The categorical predictors in GLM can be handled by one hot encoding, getting dummies via pandas library or using simple imputer techniques."],"metadata":{"id":"b-URU824o2ul"}},{"cell_type":"markdown","source":["7. What is the purpose of the design matrix in a GLM?"],"metadata":{"id":"j9Wgqb_8o24t"}},{"cell_type":"markdown","source":["~ The design matrix in a GLM contains indicator variables (ones and zeros) that indicate group membership in an various tests, or it can contain values of continuous variables."],"metadata":{"id":"4OWHBuH0o3P4"}},{"cell_type":"markdown","source":["8. How do you test the significance of predictors in a GLM?"],"metadata":{"id":"jbuQ0u3eo54a"}},{"cell_type":"markdown","source":["~ If the p-value is less than or equal to the significance level, you can conclude that there is a statistically significant association between the response variable and the term."],"metadata":{"id":"HYpZu386o6Pr"}},{"cell_type":"markdown","source":["9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"],"metadata":{"id":"-iYbd9aNo6hl"}},{"cell_type":"markdown","source":["~ In Type I, we choose the most important independent variable and it will receive the maximum amount of variation possible, Type II is used when there is no interaction and Type III is used when there is interaction."],"metadata":{"id":"R2EuFwaGo6q1"}},{"cell_type":"markdown","source":["10. Explain the concept of deviance in a GLM."],"metadata":{"id":"hJKSt6oeo60q"}},{"cell_type":"markdown","source":["~ Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It can be thought of as how much variation in the data does our proposed model account for. Therefore, the lower the deviance, the better the model."],"metadata":{"id":"HM3X5ztfo69C"}},{"cell_type":"markdown","source":["**Regression**"],"metadata":{"id":"4StmYu5To7Fk"}},{"cell_type":"markdown","source":["11. What is regression analysis and what is its purpose?"],"metadata":{"id":"v3nxrsluo7PI"}},{"cell_type":"markdown","source":["~ Regression analysis is a statistical method that shows the relationship between two or more variables.It's purpose is to estimate the effect of some explanatory variable on the dependent variable."],"metadata":{"id":"gYXiGXDYpAyq"}},{"cell_type":"markdown","source":["12. What is the difference between simple linear regression and multiple linear regression?"],"metadata":{"id":"GX4xLAnipBbG"}},{"cell_type":"markdown","source":["~ Simple linear regression has one inpendent variable and one dependent variable whereas Multiple linear regression has two or more independent variable and one dependent variable."],"metadata":{"id":"1RiFCGlZpBfG"}},{"cell_type":"markdown","source":["13. How do you interpret the R-squared value in regression?"],"metadata":{"id":"NBizlHJApBVq"}},{"cell_type":"markdown","source":["~ The interpretation of R-squared value in regression is how well the regression model explains observed data. A high R-Squared value means that many data points are close to the linear regression function line. A low R-Squared value means that the linear regression function line does not fit the data well."],"metadata":{"id":"xpZW4mKOpBQk"}},{"cell_type":"markdown","source":["14. What is the difference between correlation and regression?"],"metadata":{"id":"J9N959BnpBKQ"}},{"cell_type":"markdown","source":["~ Correlation measures the strength of a linear relationship between two variables whereas regression measures how those variables affect each other using an equation."],"metadata":{"id":"FtzCYgnFpA3U"}},{"cell_type":"markdown","source":["15. What is the difference between the coefficients and the intercept in regression?"],"metadata":{"id":"8lTmxz09pA7g"}},{"cell_type":"markdown","source":["~ In regression, coefficient is the slope which is termed as unit movement in y-axis with respest to unit movement in x-axis whereas intercept is termed as a meeting point on y-axis when value on x-axis is zero."],"metadata":{"id":"Kl6qYybepCe9"}},{"cell_type":"markdown","source":["16. How do you handle outliers in regression analysis?"],"metadata":{"id":"6khV07qSpCi6"}},{"cell_type":"markdown","source":["~ Outliers in regression analysis are handled by removing them from the observations, treating them, or using algorithms that are well-suited for dealing with such values on their own."],"metadata":{"id":"zT_9SILupCnp"}},{"cell_type":"markdown","source":["17. What is the difference between ridge regression and ordinary least squares regression?"],"metadata":{"id":"drUmAhzspE6-"}},{"cell_type":"markdown","source":["~ Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients."],"metadata":{"id":"61pamuQapE_h"}},{"cell_type":"markdown","source":["18. What is heteroscedasticity in regression and how does it affect the model?"],"metadata":{"id":"TNy5Z-1SpFE3"}},{"cell_type":"markdown","source":["~ Heteroscedasticity refers to a situation where the variance of the residuals is unequal over a range of measured values. If heteroskedasticity exists, the population used in the regression contains unequal variance, the analysis results may be invalid."],"metadata":{"id":"RKN5P804pFJK"}},{"cell_type":"markdown","source":["19. How do you handle multicollinearity in regression analysis?"],"metadata":{"id":"RAVDJHaNpGPg"}},{"cell_type":"markdown","source":["~ Multicollinearity in regression analysis can be handled by feature selection and feature extraction."],"metadata":{"id":"kiw2mvTjpGTA"}},{"cell_type":"markdown","source":["20. What is polynomial regression and when is it used?"],"metadata":{"id":"lQoW8ICwpGth"}},{"cell_type":"markdown","source":["~ Polynomial regression is a kind of linear regression in which the relationship shared between the dependent and independent variables is modeled as the nth degree of the polynomial. It is used when linear regression models may not adequately capture the complexity of the relationship."],"metadata":{"id":"tFmrdVk0pGxH"}},{"cell_type":"markdown","source":["**Loss Function**"],"metadata":{"id":"5lHs5VrxpHfy"}},{"cell_type":"markdown","source":["21. What is a loss function and what is its purpose in machine learning?"],"metadata":{"id":"gVMr208opHkJ"}},{"cell_type":"markdown","source":["~ Loss function is a method of evaluating how well your algorithm is modeling your dataset. Loss functions measure of how good your model is in terms of predicting the expected outcome."],"metadata":{"id":"x9Y5WDDMpHoA"}},{"cell_type":"markdown","source":["22. What is the difference between a convex and non-convex loss function?"],"metadata":{"id":"WqAu6YNgpT9z"}},{"cell_type":"markdown","source":["~ A convex loss function has only one global minima and no local minima whereas a non-convex loss function has one or more local minima and global minima."],"metadata":{"id":"aNGh0DnjpUEq"}},{"cell_type":"markdown","source":["23. What is mean squared error (MSE) and how is it calculated?"],"metadata":{"id":"IyRew1OupUK8"}},{"cell_type":"markdown","source":["~ Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data."],"metadata":{"id":"qBCWXXYtpUQy"}},{"cell_type":"markdown","source":["24. What is mean absolute error (MAE) and how is it calculated?"],"metadata":{"id":"_EKL_bq0pUXr"}},{"cell_type":"markdown","source":["~ Mean absolute error is an arithmetic average of the absolute errors. MAE is calculated as the sum of absolute errors divided by the sample size."],"metadata":{"id":"XfaMkPRHpUlx"}},{"cell_type":"markdown","source":["25. What is log loss (cross-entropy loss) and how is it calculated?"],"metadata":{"id":"KZY1T53upU-E"}},{"cell_type":"markdown","source":["~ Log Loss is a common evaluation metric for binary classification models. It measures the performance of a model by quantifying the difference between predicted probabilities and actual values. It is calculated by predicting class probability and comparing it to the actual class desired output 0 or 1 and a score/loss is calculated that penalizes the probability based on how far it is from the actual expected value."],"metadata":{"id":"V0KoseZQpVJT"}},{"cell_type":"markdown","source":["26. How do you choose the appropriate loss function for a given problem?"],"metadata":{"id":"0EeOuRkzpVT6"}},{"cell_type":"markdown","source":["~ Most machine learning algorithms use some sort of loss function in the process of optimization or finding the best parameters  for your data. The choice of the loss function is directly related to the activation function used in the output layer of your neural network."],"metadata":{"id":"LcdrcxEepVh7"}},{"cell_type":"markdown","source":["27. Explain the concept of regularization in the context of loss functions."],"metadata":{"id":"S286NdZepVtP"}},{"cell_type":"markdown","source":["~ Regularization in the context of loss function refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting."],"metadata":{"id":"Rey8i0TRpWnz"}},{"cell_type":"markdown","source":["28. What is Huber loss and how does it handle outliers?"],"metadata":{"id":"0fSXRQ4hpXFc"}},{"cell_type":"markdown","source":["~ Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. To detect outliers it assigns less weight to observations identified as outliers."],"metadata":{"id":"RQ_etGLkpXP5"}},{"cell_type":"markdown","source":["29. What is quantile loss and when is it used?"],"metadata":{"id":"7pKJOHczpXas"}},{"cell_type":"markdown","source":["~ Quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls.It is used when Machine learning algorithms are aiming to predict a particular variable quantile."],"metadata":{"id":"yy4nLKFbpXwQ"}},{"cell_type":"markdown","source":["30. What is the difference between squared loss and absolute loss?"],"metadata":{"id":"n2n4juyipYFI"}},{"cell_type":"markdown","source":["~ The Squared loss is a measure of the quality of an estimator, it is always positive, and values which are closer to zero are better whereas the absolute loss is the second moment of the error, and includes both the variance of the estimator and its bias."],"metadata":{"id":"zgG8JcRSpYsF"}},{"cell_type":"markdown","source":["**Optimizer (GD)**"],"metadata":{"id":"PVQnHu8cpY2J"}},{"cell_type":"markdown","source":["31. What is an optimizer and what is its purpose in machine learning?"],"metadata":{"id":"Nyy3MEEzpZBW"}},{"cell_type":"markdown","source":["~ An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. It's purpose is to  assist in improving the accuracy and reduces the total loss."],"metadata":{"id":"ZS04274LpZMS"}},{"cell_type":"markdown","source":["32. What is Gradient Descent (GD) and how does it work?"],"metadata":{"id":"eMi1kZMIpZWy"}},{"cell_type":"markdown","source":["~ Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning models by means of minimizing errors between actual and expected results. It works by taking the fastest route towards the minimum point from each step to converge fast. It is done by taking the partial derivative at each step to find the direction towards the local minimum."],"metadata":{"id":"F0a23OQnpZg9"}},{"cell_type":"markdown","source":["33. What are the different variations of Gradient Descent?"],"metadata":{"id":"dAgTWNW5pZ7e"}},{"cell_type":"markdown","source":["~ Different variations of Gradient Descent are batch gradient descent, stochastic gradient descent and mini-batch gradient descent."],"metadata":{"id":"BurBSRdrpaKq"}},{"cell_type":"markdown","source":["34. What is the learning rate in GD and how do you choose an appropriate value?"],"metadata":{"id":"12kFD9Gopadk"}},{"cell_type":"markdown","source":["~ Learning rate determines how fast or slow we will move towards the optimal weights. In order for Gradient Descent to work, we must set the learning rate to an appropriate value. We take the value of the learning rate to be 0.1, 0.01 or 0.001. The value of the step should not be too big as it can skip the minimum point and thus the optimisation can fail."],"metadata":{"id":"1yp140UMpasu"}},{"cell_type":"markdown","source":["35. How does GD handle local optima in optimization problems?"],"metadata":{"id":"m2v8Kypupa8G"}},{"cell_type":"markdown","source":["~ By putting a fraction of the past weight update to the current weight update. This helps prevent the optimization problem from getting stuck in local minima or by using variant of GD called SGD (stochastic gradient descent)."],"metadata":{"id":"okFrCCZXpbLR"}},{"cell_type":"markdown","source":["36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"],"metadata":{"id":"KBzWRTXepbZR"}},{"cell_type":"markdown","source":["~ Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm that is used for optimizing machine learning models. It addresses the computational inefficiency of traditional Gradient Descent methods when dealing with large datasets in machine learning projects. In SGD, instead of using the entire dataset for each iteration, only a single random training example or a small batch is selected to calculate the gradient and update the model parameters."],"metadata":{"id":"kVXGJDqkpboK"}},{"cell_type":"markdown","source":["37. Explain the concept of batch size in GD and its impact on training."],"metadata":{"id":"wstJpX9hpb3m"}},{"cell_type":"markdown","source":["~ The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. Batch size is important because it affects both the training time and the generalization of the model. A smaller batch size allows the model to learn from each individual example but takes longer to train. A larger batch size trains faster but may result in the model not capturing the nuances in the data."],"metadata":{"id":"KlgPCvVapcHh"}},{"cell_type":"markdown","source":["38. What is the role of momentum in optimization algorithms?"],"metadata":{"id":"BA4GmQy9pckm"}},{"cell_type":"markdown","source":["~ Momentum is a strategy for accelerating the convergence of the optimization process by including a momentum element in the update rule. This momentum factor assists the optimizer in continuing to go in the same direction even if the gradient changes direction or becomes zero."],"metadata":{"id":"aSHgTKJDpcyo"}},{"cell_type":"markdown","source":["39. What is the difference between batch GD, mini-batch GD, and SGD?"],"metadata":{"id":"z6qkuyGWpdCU"}},{"cell_type":"markdown","source":["~ In Gradient Desecent whole training data is used to update the network's parameters.\n","\n","~ In Stochastic Gradient Descent, we update the parameters after every single observation and we know that every time the weights are updated it is known as an iteration.\n","\n","~ In Mini-batch Gradient Descent, we take a subset of data and update the parameters based on every subset."],"metadata":{"id":"9fV1_vdGpdS4"}},{"cell_type":"markdown","source":["40. How does the learning rate affect the convergence of GD?"],"metadata":{"id":"UgP-aHbNpdg1"}},{"cell_type":"markdown","source":["~ The learning rate determines how big the step would be on each iteration. If learning rate is very small, it would take long time to converge and become computationally expensive. If learning rate is large, it may fail to converge and skip the minimum."],"metadata":{"id":"ESqS44T9pdv4"}},{"cell_type":"markdown","source":["**Regularization**"],"metadata":{"id":"pop88xYqpd-k"}},{"cell_type":"markdown","source":["41. What is regularization and why is it used in machine learning?"],"metadata":{"id":"Dlacwc_6peNG"}},{"cell_type":"markdown","source":["~ Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."],"metadata":{"id":"sDF55c-qpecP"}},{"cell_type":"markdown","source":["42. What is the difference between L1 and L2 regularization?"],"metadata":{"id":"R1j-teW1peqB"}},{"cell_type":"markdown","source":["~ L1 Regularization adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function.\n","\n","~ L2 Regularization adds the “squared magnitude” of the coefficient as the penalty term to the loss function."],"metadata":{"id":"ayRcefWkpe5L"}},{"cell_type":"markdown","source":["43. Explain the concept of ridge regression and its role in regularization."],"metadata":{"id":"C6JMdNVspfH8"}},{"cell_type":"markdown","source":["~ Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. It is used in regularization for reducing overfitting."],"metadata":{"id":"qJCZjLg6pfWq"}},{"cell_type":"markdown","source":["44. What is the elastic net regularization and how does it combine L1 and L2 penalties?"],"metadata":{"id":"kb2si7mmpfkg"}},{"cell_type":"markdown","source":["~ Elastic net regression is a hybrid of lasso and ridge regression that uses combination of the L1 and L2 norms as the penalty term. This allows it to balance between feature selection and feature preservation, and to deal with situations where lasso and ridge regression may fail. It combines L1 and L1 penalties linearly."],"metadata":{"id":"LjvkawaGpf1p"}},{"cell_type":"markdown","source":["45. How does regularization help prevent overfitting in machine learning models?"],"metadata":{"id":"VATdMQqCpgGL"}},{"cell_type":"markdown","source":["~ Regularization tunes the loss function by adding a penalty term, that prevents excessive fluctuation of the coefficients. Thereby, reducing the chances of overfitting."],"metadata":{"id":"aUgBHUxxpgXi"}},{"cell_type":"markdown","source":["46. What is early stopping and how does it relate to regularization?"],"metadata":{"id":"DIaKBhNQpgn6"}},{"cell_type":"markdown","source":["~ Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Early stopping belongs to the class of methods of regularization."],"metadata":{"id":"_NyyGthvpg4O"}},{"cell_type":"markdown","source":["47. Explain the concept of dropout regularization in neural networks."],"metadata":{"id":"L03hm1X1phJx"}},{"cell_type":"markdown","source":["~ Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer."],"metadata":{"id":"8e3eOtLzphaU"}},{"cell_type":"markdown","source":["48. How do you choose the regularization parameter in a model?"],"metadata":{"id":"GQmDwFcMphqv"}},{"cell_type":"markdown","source":["~ On the training set, we estimate several different Ridge, Lasso or Elastic net regressions, with different values of the regularization parameter and on the validation set, we choose the best model with best regularization parameter which gives the lowest MSE on the validation set by hyperparameter tuning."],"metadata":{"id":"fpyvE_8mph7X"}},{"cell_type":"markdown","source":["49. What is the difference between feature selection and regularization?"],"metadata":{"id":"-w4Wvh3KpiKm"}},{"cell_type":"markdown","source":["~ Feature selection removes the dimensions e.g. columns from the input data and results in a reduced data set for model inference. Regularization is where we are constraining the solution space while doing optimization."],"metadata":{"id":"AwEtOWhZpiZ0"}},{"cell_type":"markdown","source":["50. What is the trade-off between bias and variance in regularized models?"],"metadata":{"id":"9avej1VZpi0_"}},{"cell_type":"markdown","source":["~ Bias and variance are inversely connected. If we decrease the variance, it will increase the bias. If we decrease the bias, it will increase the variance."],"metadata":{"id":"svd4U-C3pjLp"}},{"cell_type":"markdown","source":["**SVM**"],"metadata":{"id":"sjb2sPUypjbC"}},{"cell_type":"markdown","source":["51. What is Support Vector Machines (SVM) and how does it work?"],"metadata":{"id":"WUkrhiWjpjq5"}},{"cell_type":"markdown","source":["~ Support Vector Machine (SVM) is a powerful machine learning algorithm used for linear or nonlinear classification, regression, and even outlier detection tasks. It works on the objective of the SVM algorithm, that is to find the optimal hyperplane in an N-dimensional space that can separate the data points in different classes in the feature space. The hyperplane tries that the margin between the closest points of different classes should be as maximum as possible."],"metadata":{"id":"-uV6nGGdpj5q"}},{"cell_type":"markdown","source":["52. How does the kernel trick work in SVM?"],"metadata":{"id":"iAp6hqH_pcVy"}},{"cell_type":"markdown","source":["~ Kernel trick transforms the training set of data so that a non-linear decision surface is able to transform to a linear equation in a higher number of dimension spaces."],"metadata":{"id":"B2p4Q8FgpYjT"}},{"cell_type":"markdown","source":["53. What are support vectors in SVM and why are they important?"],"metadata":{"id":"rQg3ZfhFpYZm"}},{"cell_type":"markdown","source":["~ Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Support vectors are important because using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."],"metadata":{"id":"T1N1V_7mpYPW"}},{"cell_type":"markdown","source":["54. Explain the concept of the margin in SVM and its impact on model performance."],"metadata":{"id":"H3AucUCVpX7M"}},{"cell_type":"markdown","source":["~ Margin in SVM is the distance between few closest points of different classes of datapoints. If the Margin is more then the set datapoints will be more clearly seperated and model prediction and performance will be better and if Margin is less, it accuracy model prediction and performance."],"metadata":{"id":"Tuk9LbhCpXl_"}},{"cell_type":"markdown","source":["55. How do you handle unbalanced datasets in SVM?"],"metadata":{"id":"iDNXRxH_pW80"}},{"cell_type":"markdown","source":["~ A popular algorithm for handling is Penalized-SVM. During training, we can use the argument class_weight='balanced' to penalize mistakes on the minority class by an amount proportional to how under-represented it is."],"metadata":{"id":"u10wb_GnpWy7"}},{"cell_type":"markdown","source":["56. What is the difference between linear SVM and non-linear SVM?"],"metadata":{"id":"ny5Kt7fNpWc-"}},{"cell_type":"markdown","source":["~ When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data.\n","\n","~ When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."],"metadata":{"id":"YLhyXS_6pWH0"}},{"cell_type":"markdown","source":["57. What is the role of C-parameter in SVM and how does it affect the decision boundary?"],"metadata":{"id":"APLChZF6pV77"}},{"cell_type":"markdown","source":["~ The C-parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly."],"metadata":{"id":"CZuhzQiMpUyz"}},{"cell_type":"markdown","source":["58. Explain the concept of slack variables in SVM."],"metadata":{"id":"l6kkBWQTpHse"}},{"cell_type":"markdown","source":["~ Slack variables are introduced to allow certain constraints to be violated. That is, certain training points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible we want their penetration of the margin to be as small as possible."],"metadata":{"id":"52-JFaNFpTJc"}},{"cell_type":"markdown","source":["59. What is the difference between hard margin and soft margin in SVM?"],"metadata":{"id":"FV2_igumpTrp"}},{"cell_type":"markdown","source":["~ In Marginal plane, if there are no error points then it is called Hard margin and if error points are present in the marginal plane then it is called Soft Margin."],"metadata":{"id":"IThFXeqapTw6"}},{"cell_type":"markdown","source":["60. How do you interpret the coefficients in an SVM model?"],"metadata":{"id":"EJcR7vrvpTh0"}},{"cell_type":"markdown","source":["~ C parameter in SVM is Penalty parameter of the error term. You can consider it as the degree of correct coefficient that the algorithm has to meet or the degree of optimization the the SVM has to meet. For greater values of C, there is no way that SVM optimizer can misclassify any single point."],"metadata":{"id":"iPSGMQrzpTan"}},{"cell_type":"markdown","source":["**Decision Trees**"],"metadata":{"id":"WN6EXZAVpTSP"}},{"cell_type":"markdown","source":["61. What is a decision tree and how does it work?"],"metadata":{"id":"kKkkqa_IpTCo"}},{"cell_type":"markdown","source":["~ Decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes. It follows a tree-like model of decisions and their possible consequences. The algorithm works by recursively splitting the data into subsets based on the most significant feature at each node of the tree"],"metadata":{"id":"hIWS13BApS5R"}},{"cell_type":"markdown","source":["62. How do you make splits in a decision tree?"],"metadata":{"id":"p81LQ7XGpSyA"}},{"cell_type":"markdown","source":["~ For each split, calculate the entropy of each child node independently.\n","\n","~ Calculate the entropy of each split using the weighted average entropy of child nodes.\n","\n","~ Choose the split with the lowest entropy or the greatest gain in information.\n","\n","~ Repeat these steps to obtain homogeneous split nodes."],"metadata":{"id":"7SEPeGhppSq1"}},{"cell_type":"markdown","source":["63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?"],"metadata":{"id":"Oc-vS-2UpSkD"}},{"cell_type":"markdown","source":["~ In the context of Decision Trees, Entropy is a measure of disorder or impurity in a node and Gini Index aims to decrease the impurities from the root nodes to the leaf nodes of a decision tree model. They are used to find Information Gain, further this information gain is used to decide best split in Decision tree."],"metadata":{"id":"lx64eX5SpScK"}},{"cell_type":"markdown","source":["64. Explain the concept of information gain in decision trees."],"metadata":{"id":"ALB79_QZpSUM"}},{"cell_type":"markdown","source":["~ Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split, the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."],"metadata":{"id":"L1W0sZy4pSLr"}},{"cell_type":"markdown","source":["65. How do you handle missing values in decision trees?"],"metadata":{"id":"L22s3MMkpICz"}},{"cell_type":"markdown","source":["~ Decision Tree can automatically handle missing values. Decision Tree is usually robust to outliers and can handle them automatically."],"metadata":{"id":"MeGrO3qUpSAV"}},{"cell_type":"markdown","source":["66. What is pruning in decision trees and why is it important?"],"metadata":{"id":"1fFe-jS1pSFL"}},{"cell_type":"markdown","source":["~ Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are unrelevant and unnecessary. Pruning is important as it reduces the complexity of the decision tree being long and avoiding unnecessary branching of the decision tree."],"metadata":{"id":"HhAMTcaspR4t"}},{"cell_type":"markdown","source":["67. What is the difference between a classification tree and a regression tree?"],"metadata":{"id":"KOGquHuRpRxa"}},{"cell_type":"markdown","source":["~ Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous."],"metadata":{"id":"P6Ght_ZDpRqw"}},{"cell_type":"markdown","source":["68. How do you interpret the decision boundaries in a decision tree?"],"metadata":{"id":"GHCuZVOLpRjQ"}},{"cell_type":"markdown","source":["~ The first node of the tree called the root node contains the number of instances of all the classes respectively. Basically, we have to draw a line called decision boundary that separates the instances of different classes into different regions called decision regions."],"metadata":{"id":"qkNjD9SzpRa2"}},{"cell_type":"markdown","source":["69. What is the role of feature importance in decision trees?"],"metadata":{"id":"vdw2LyAapRQj"}},{"cell_type":"markdown","source":["~ Feature importance refers to technique that assigns a score to features based on how significant they are at predicting a target variable. The scores are calculated on the weighted Gini indices. Easy way to obtain the scores is by using the feature_importances_ attribute from the trained decision tree model."],"metadata":{"id":"easoavYXpIIT"}},{"cell_type":"markdown","source":["70. What are ensemble techniques and how are they related to decision trees?"],"metadata":{"id":"lrHq7SNVpREl"}},{"cell_type":"markdown","source":["~ Ensemble techniques are the methods that create multiple models and then combine them to produce improved results. Ensemble methods in machine learning usually produce more accurate solutions than a single model would. Ensemble techniques are related to decision trees as tehy use one or more decision trees to predict the outcome or accuracy of any machine learning models."],"metadata":{"id":"bxBkkgH8pQ9L"}},{"cell_type":"markdown","source":["**Ensemble Techniques**"],"metadata":{"id":"ABfEhzRkpQ2k"}},{"cell_type":"markdown","source":["71. What are ensemble techniques in machine learning?"],"metadata":{"id":"6dDWU0XfpKls"}},{"cell_type":"markdown","source":["~ Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods in machine learning usually produce more accurate solutions than a single model would."],"metadata":{"id":"cfd7eByQpQqR"}},{"cell_type":"markdown","source":["72. What is bagging and how is it used in ensemble learning?"],"metadata":{"id":"_5dlY8jJpQip"}},{"cell_type":"markdown","source":["~ Bagging is a homogeneous weak learners model that learns from each other independently in parallel and combines them for determining the model average. It is used in ensemble learning to reduce the variance.\n","\n","~ Steps to use Bagging:\n","\n","Step 1: Multiple subsets are created from the original data set with equal tuples, selecting observations with replacement.\n","\n","Step 2: A base model is created on each of these subsets.\n","\n","Step 3: Each model is learned in parallel with each training set and independent of each other.\n","\n","Step 4: The final predictions are determined by combining the predictions from all the models."],"metadata":{"id":"dt3SGu-hpQb8"}},{"cell_type":"markdown","source":["73. Explain the concept of bootstrapping in bagging."],"metadata":{"id":"7il4PR83pQUW"}},{"cell_type":"markdown","source":["~ Bootstrap Aggregating, also known as bootstrapping is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It decreases the variance and helps to avoid overfitting."],"metadata":{"id":"aS7e5YMWMSJQ"}},{"cell_type":"markdown","source":["74. What is boosting and how does it work?"],"metadata":{"id":"9WqClRILpQEI"}},{"cell_type":"markdown","source":["~ Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added."],"metadata":{"id":"vyZMf0dlpP6g"}},{"cell_type":"markdown","source":["75. What is the difference between AdaBoost and Gradient Boosting?"],"metadata":{"id":"S9h57QY6pPvN"}},{"cell_type":"markdown","source":["~ Adaboost is computed with a specific loss function and becomes more rigid when comes to few iterations. But in Gradient boosting, it assists in finding the proper solution to additional iteration modeling problem as it is built with some generic features. From this, it is noted that Gradient boosting is more flexible when compared to AdaBoost because of its fixed loss function values."],"metadata":{"id":"sYgp2XptpMtN"}},{"cell_type":"markdown","source":["76. What is the purpose of random forests in ensemble learning?"],"metadata":{"id":"5UxqlU8QpPid"}},{"cell_type":"markdown","source":["~ Random forest algorithm is an ensemble learning technique combining numerous classifiers to enhance a model's performance. Random Forest is a supervised machine-learning algorithm made up of several decision trees. By default Random forest generates hundred decision trees which can be changed set as per required parameter value."],"metadata":{"id":"3wBTtCIVpMxE"}},{"cell_type":"markdown","source":["77. How do random forests handle feature importance?"],"metadata":{"id":"bvXp3h4_pPZF"}},{"cell_type":"markdown","source":["~ The more a feature decreases the impurity, the more important the feature is. The final feature importance, at the Random Forest level, is it's average over all the trees. The sum of the feature's importance value on each trees is calculated and divided by the total number of trees."],"metadata":{"id":"i0c-yMOgpPNH"}},{"cell_type":"markdown","source":["78. What is stacking in ensemble learning and how does it work?"],"metadata":{"id":"GU0Li5G7pPAu"}},{"cell_type":"markdown","source":["~ Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking uses predictions for multiple nodes for example kNN, decision trees, or SVM to build a new model. This final model is used for making predictions on the test dataset."],"metadata":{"id":"nFrtCNkOpM08"}},{"cell_type":"markdown","source":["79. What are the advantages and disadvantages of ensemble techniques?"],"metadata":{"id":"iwG-w5JjpM5P"}},{"cell_type":"markdown","source":["~ Advantages : Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data.\n","\n","~ Disadvantages : Ensembling is less interpretable, the output of the ensembled model is hard to predict and explain.\n","The art of ensembling is hard to learn and any wrong selection can lead to lower predictive accuracy than an individual model.\n","Ensembling is expensive in terms of both time and space."],"metadata":{"id":"ZDpJeHumpNBk"}},{"cell_type":"markdown","source":["80. How do you choose the optimal number of models in an ensemble?"],"metadata":{"id":"6IUx-v0CpLMe"}},{"cell_type":"markdown","source":["~ Steps for optimal number of models in ensemble :-\n","\n","Step 1 : Find the KS of individual models.\n","\n","Step 2: Index all the models for easy access.\n","\n","Step 3: Choose the first two models as the initial selection and set a correlation limit.\n","\n","Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model.\n","\n","Step 5: Time to check the performance of individual sequential combination.\n","\n","Step 6: Choose the combination of models where the performance peaks."],"metadata":{"id":"Itx3Ez_apMjp"}}]}