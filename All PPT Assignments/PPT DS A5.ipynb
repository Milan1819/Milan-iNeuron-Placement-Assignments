{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOAHNUG2MIr0fYqmRBQEU7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Naive Approach**"],"metadata":{"id":"7Yq1olFVZdBx"}},{"cell_type":"markdown","source":["1. What is the Naive Approach in machine learning?"],"metadata":{"id":"MHOWJVDWXyMt"}},{"cell_type":"markdown","source":["~ Naive Approach or Naive Bayes theorem works on the principle of Bayes theorem. Bayes theorem works on conditional probability. Conditional probability is the probability that something will happen, given that something else has already occurred. The conditional probability can give us the probability of an event using its prior knowledge."],"metadata":{"id":"4Xq0p9l1ZyNh"}},{"cell_type":"markdown","source":["2. Explain the assumptions of feature independence in the Naive Approach."],"metadata":{"id":"gdCnGy-AXz58"}},{"cell_type":"markdown","source":["~ The assumption of feature independence in Naive Bayes simplifies the computation and makes the algorithm more efficient. By assuming independence, the joint probability of features given the class can be computed as the product of individual feature probabilities."],"metadata":{"id":"JiTzF7GTX0G1"}},{"cell_type":"markdown","source":["3. How does the Naive Approach handle missing values in the data?"],"metadata":{"id":"bwSgpMETX1NB"}},{"cell_type":"markdown","source":["~ Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value."],"metadata":{"id":"BcJmeUbzX1Y6"}},{"cell_type":"markdown","source":["4. What are the advantages and disadvantages of the Naive Approach?"],"metadata":{"id":"AYIG92lLX1qo"}},{"cell_type":"markdown","source":["~ Advantages:\n","\n","• It is simple and easy to implement.\n","\n","• It doesn't require as much training data.\n","\n","• It handles both continuous and discrete data.\n","\n","• It is highly scalable with the number of predictors and data points.\n","\n","• It is fast and can be used to make real-time predictions.\n","\n","• Naive Bayes is better suited for categorical input variables than numerical variables.\n","\n","~ Disadvantages:\n","\n","• Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life. This limits the applicability of this algorithm in real-world use cases.\n","\n","• This algorithm faces the zero-frequency problem where it assigns zero probability to a categorical variable whose category in the test data set wasn’t available in the training dataset. It would be best if you used a smoothing technique to overcome this issue.\n","\n","• Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously."],"metadata":{"id":"B7fZl_uDX161"}},{"cell_type":"markdown","source":["5. Can the Naive Approach be used for regression problems? If yes, how?"],"metadata":{"id":"ihd3H_d5X2JT"}},{"cell_type":"markdown","source":["~ With some modifications, Naive Approach can also be used for solving regression problems.is a type of conditional modeling in which the mean of one variable is described by a linear combination of other variables, with the goal of obtaining the posterior probability of the regression coefficients and ultimately allowing the out-of-sample prediction of the regressand conditional on observed values of the regressors."],"metadata":{"id":"S1UZzz5oX2XY"}},{"cell_type":"markdown","source":["6. How do you handle categorical features in the Naive Approach?"],"metadata":{"id":"EgVemcFPX2kL"}},{"cell_type":"markdown","source":["~ The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution. CategoricalNB assumes that the sample matrix\n"," is encoded (for instance with the help of OrdinalEncoder) such that all categories for each feature  are represented with numbers with the number of available categories of feature."],"metadata":{"id":"vW9rtTwuX2wx"}},{"cell_type":"markdown","source":["7. What is Laplace smoothing and why is it used in the Naive Approach?"],"metadata":{"id":"0ijrLDHlX2_n"}},{"cell_type":"markdown","source":["~ Laplace smoothing is an algorithm to smooth a polygonal mesh. For each vertex in a mesh, a new position is chosen based on local information and the vertex is moved there. Laplace smoothing in helps to tackle the problem of zero probability in the Naïve Bayes machine learning algorithm."],"metadata":{"id":"c44pF4X-X3Om"}},{"cell_type":"markdown","source":["8. How do you choose the appropriate probability threshold in the Naive Approach?"],"metadata":{"id":"E0PjC2UgX3cq"}},{"cell_type":"markdown","source":["~ In Naive Approach, you can set the thresholds as a value in the range [0, 1], such that they sum to 1. This will get you the desired rule of classify as True if the probability is over threshold, otherwise classify as False."],"metadata":{"id":"-wXsKMwqX3rC"}},{"cell_type":"markdown","source":["9. Give an example scenario where the Naive Approach can be applied."],"metadata":{"id":"0fApPxBnX36s"}},{"cell_type":"markdown","source":["~ Naive approach forecasting example, A naïve forecast is one in which the forecast for a particular period is just the same as the preceding period's value. For example, you sold 100 Bags last month and estimate that you would sell 100 Bags again this month. It is used for various purposes such as  Credit Scoring, medical data classification, Text classification such as Spam filtering and Sentiment analysis."],"metadata":{"id":"HD2CxM2MX4JX"}},{"cell_type":"markdown","source":["**KNN**"],"metadata":{"id":"rY7fuvttX4X5"}},{"cell_type":"markdown","source":["10. What is the K-Nearest Neighbors (KNN) algorithm?"],"metadata":{"id":"RRDzwj90X4mJ"}},{"cell_type":"markdown","source":["~ KNN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n","KNN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm."],"metadata":{"id":"5S3JWNM6X40c"}},{"cell_type":"markdown","source":["11. How does the KNN algorithm work?"],"metadata":{"id":"j1oyquH9X5CM"}},{"cell_type":"markdown","source":["~ KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label in the case of classification or averages the labels in the case of regression."],"metadata":{"id":"0WrbQ29DX5Q1"}},{"cell_type":"markdown","source":["12. How do you choose the value of K in KNN?"],"metadata":{"id":"xThuqNdDX5eh"}},{"cell_type":"markdown","source":["~ The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better."],"metadata":{"id":"MYvYtkr7X5sn"}},{"cell_type":"markdown","source":["13. What are the advantages and disadvantages of the KNN algorithm?"],"metadata":{"id":"CRJj2QfSX55r"}},{"cell_type":"markdown","source":["~ Advantages :\n","\n","• It's easy to understand and simple to implement.\n","\n","• It can be used for both classification and regression problems.\n","\n","• It's ideal for non-linear data since there's no assumption about underlying data.\n","\n","• High accuracy – you do not need to compare with better-supervised learning models.\n","\n","~ Disadvantages :\n","\n","• Accuracy depends on the quality of the data.\n","\n","• With large data, the prediction stage might be slow.\n","\n","• Sensitive to the scale of the data and irrelevant features.\n","\n","• Require high memory – need to store all of the training data.\n","\n","• Given that it stores all of the training, it can be computationally expensive."],"metadata":{"id":"Gan3sEG2X6Gh"}},{"cell_type":"markdown","source":["14. How does the choice of distance metric affect the performance of KNN?"],"metadata":{"id":"w_9ezYODX6Un"}},{"cell_type":"markdown","source":["~ KNN algorithm calculates the distances between the point and points in the training data set. Usually, the Euclidean distance is used as the distance metric. As KNN is a distance based algorithm, the choice of distance measures affect the prediction and performance of KNN models."],"metadata":{"id":"0d1BZMRqX6it"}},{"cell_type":"markdown","source":["15. Can KNN handle imbalanced datasets? If yes, how?"],"metadata":{"id":"DZ_o02OKX6wD"}},{"cell_type":"markdown","source":["~ In k Nearest Neighbor (kNN) algorithm, a query instance is classified based on the most frequent class of its nearest neighbors among the training instances. In imbalanced datasets, kNN becomes biased towards the majority instances of the training space."],"metadata":{"id":"SCBfxYRMX69U"}},{"cell_type":"markdown","source":["16. How do you handle categorical features in KNN?"],"metadata":{"id":"A8vBvDWkX7Lx"}},{"cell_type":"markdown","source":["~ In KNN algorithm, categorical features can be handled by converting them to numerical features with the techniques like One-Hot encoding or label encoding. Once the categorical data is converted to numerical data KNN can be applied as usual."],"metadata":{"id":"kVKkpJZfX7Z2"}},{"cell_type":"markdown","source":["17. What are some techniques for improving the efficiency of KNN?"],"metadata":{"id":"TFFelgUxX7yg"}},{"cell_type":"markdown","source":["~ Re-scaling, reduce curse of dimensionality, PCA and KD-tree are some techniques for improving the efficiency of KNN."],"metadata":{"id":"9pfS4nbdX8BV"}},{"cell_type":"markdown","source":["18. Give an example scenario where KNN can be applied."],"metadata":{"id":"sCGqZibjX8Oa"}},{"cell_type":"markdown","source":["~ With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP' etc. KNN algorithm is widely used in Text mining, Agriculture, Finance, Medical field, Facial recognition, Recommendation systems (Amazon, Hulu, Netflix, etc)."],"metadata":{"id":"r4bT3AnpX8a3"}},{"cell_type":"markdown","source":["**Clustering**"],"metadata":{"id":"0QLVoL9SX8nb"}},{"cell_type":"markdown","source":["19. What is clustering in machine learning?"],"metadata":{"id":"Cm8dY9YXX80E"}},{"cell_type":"markdown","source":["~ Clustering is a unsupervised machine learning technique which has the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them."],"metadata":{"id":"jAqPBW6BX9CW"}},{"cell_type":"markdown","source":["20. Explain the difference between hierarchical clustering and k-means clustering."],"metadata":{"id":"sMwgUAuHX9QB"}},{"cell_type":"markdown","source":["~ K-means clustering, using a pre-specified number of clusters, the method assigns records to each cluster to find the mutually exclusive cluster of spherical shape based on distance whereas Hierarchical clustering is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."],"metadata":{"id":"-hxsdnAzX9co"}},{"cell_type":"markdown","source":["21. How do you determine the optimal number of clusters in k-means clustering?"],"metadata":{"id":"IjkRGqLYX9pU"}},{"cell_type":"markdown","source":["~ By using Elbow Curve Method we determine optimal number of clusters in k-mean clustering.\n","The elbow method runs k-means clustering (kmeans number of clusters) on the dataset for a range of values of k. In the elbow method, we plot mean distance and look for the elbow point where the rate of decrease shifts. For each k, calculate the total within-cluster sum of squares (WSS)."],"metadata":{"id":"2AozaqLHX92h"}},{"cell_type":"markdown","source":["22. What are some common distance metrics used in clustering?"],"metadata":{"id":"i1W5ZiETX-E8"}},{"cell_type":"markdown","source":["~ Euclidean Distance, Manhattan Distance, Jaccard Index, Minkowski distance, Cosine Index are some common distance metrics used in clustering."],"metadata":{"id":"_25msDK4X-TJ"}},{"cell_type":"markdown","source":["23. How do you handle categorical features in clustering?"],"metadata":{"id":"n00xVlcDX-il"}},{"cell_type":"markdown","source":["~ K- Modes clustering, One-Hot encoding and mixed clustering are the ways to handle categorical features in clustering."],"metadata":{"id":"dA6K2Eh3VsxN"}},{"cell_type":"markdown","source":["24. What are the advantages and disadvantages of hierarchical clustering?"],"metadata":{"id":"qjiwdB72Vtji"}},{"cell_type":"markdown","source":["~ Advantages :\n","\n","• We can obtain the optimal number of clusters from the model itself, human intervention not required.\n","\n","• Dendrograms help us in clear visualization, which is practical and easy to understand.\n","\n","~ Disadvantages :\n","\n","• Not suitable for large datasets due to high time and space complexity.\n","\n","• There is no mathematical objective for Hierarchical clustering.\n","\n","• All the approaches to calculate the similarity between clusters has their own disadvantages."],"metadata":{"id":"OQuofgdpVt-W"}},{"cell_type":"markdown","source":["25. Explain the concept of silhouette score and its interpretation in clustering."],"metadata":{"id":"XU7vJ-FKVuKn"}},{"cell_type":"markdown","source":["~ The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette score ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters."],"metadata":{"id":"8FINgxZxVuVn"}},{"cell_type":"markdown","source":["26. Give an example scenario where clustering can be applied."],"metadata":{"id":"L7sQJSXSVupA"}},{"cell_type":"markdown","source":["~ Retail Marketing : Retail companies often use clustering to identify groups of households that are similar to each other.For example, a retail company may collect the following information on households:\n","\n","• Household income\n","\n","• Household size\n","\n","• Head of household Occupation\n","\n","• Distance from nearest urban area\n","\n","They can then feed these variables into a clustering algorithm to perhaps identify the following clusters:\n","\n","Cluster 1: Small family, high spenders\n","\n","Cluster 2: Larger family, high spenders\n","\n","Cluster 3: Small family, low spenders\n","\n","Cluster 4: Large family, low spenders"],"metadata":{"id":"PMkzXP8QVuzf"}},{"cell_type":"markdown","source":["**Anomaly Detection**"],"metadata":{"id":"JPdZsZKDVu9J"}},{"cell_type":"markdown","source":["27. What is anomaly detection in machine learning?"],"metadata":{"id":"nHYny_RFVvIF"}},{"cell_type":"markdown","source":["~ Anomaly Detection is the technique of identifying rare events or observations which can raise suspicions by being statistically different from the rest of the observations. Such anomalous behaviour typically translates to some kind of a problem like a credit card fraud, failing machine in a server, a cyber attack, etc."],"metadata":{"id":"p4nPjDPcVvSW"}},{"cell_type":"markdown","source":["28. Explain the difference between supervised and unsupervised anomaly detection."],"metadata":{"id":"LbIx6p6DVxm6"}},{"cell_type":"markdown","source":["~ Supervised anomaly detection requires a labeled dataset containing both normal and anomalous samples to construct a predictive model to classify future data points whereas Unsupervised anomaly detection assumes two things about the data; Only a small percentage of data is anomalous and Any anomaly is statistically different from the normal samples. Based on the above assumptions, the data is then clustered using a similarity measure and the data points which are far off from the cluster are considered to be anomalies."],"metadata":{"id":"Zht31AERVxsH"}},{"cell_type":"markdown","source":["29. What are some common techniques used for anomaly detection?"],"metadata":{"id":"bpWh2cscVxws"}},{"cell_type":"markdown","source":["~ Density-based techniques, Cluster analysis based techniques, Bayesian Networks, Neural networks, autoencoders, LSTM networks, Support vector machines, Hidden Markov models are some common techniques used for anomaly detection."],"metadata":{"id":"vHC-SycpVyIX"}},{"cell_type":"markdown","source":["30. How does the One-Class SVM algorithm work for anomaly detection?"],"metadata":{"id":"w7WWSnp5VyLw"}},{"cell_type":"markdown","source":["~ One-Class SVM algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. It finds a hyper-plane that separates the given dataset from the origin such that the hyperplane is as close to the datapoints as possible. The RBF kernel is used to fit a non-linear boundary around the dense region of the dataset separating the remaining points as outliers."],"metadata":{"id":"_HPsR9g3VyDG"}},{"cell_type":"markdown","source":["31. How do you choose the appropriate threshold for anomaly detection?"],"metadata":{"id":"wn9tBN8AVx1W"}},{"cell_type":"markdown","source":["~ Construct a train set using a large sample of observations without anomalies.\n","Take a smaller sample of observations containing anomalies (manually labelled) and use it to construct a validation and test set.\n","Now train your anomaly detection model while tuning the threshold using the validation set and additionally using the test set to evaluate your model."],"metadata":{"id":"5sFgp4w4Vx_T"}},{"cell_type":"markdown","source":["32. How do you handle imbalanced datasets in anomaly detection?"],"metadata":{"id":"LQC2difdVxHP"}},{"cell_type":"markdown","source":["~ Re-sampling, Random Undersampling and Over sampling are some techniques used to handle imbalaced datasets in anomaly detection."],"metadata":{"id":"GhdWNtNIVxK_"}},{"cell_type":"markdown","source":["33. Give an example scenario where anomaly detection can be applied."],"metadata":{"id":"nDbOtA9KVxQT"}},{"cell_type":"markdown","source":["~ A credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer."],"metadata":{"id":"l5_YkBUEVwNx"}},{"cell_type":"markdown","source":["**Dimension Reduction**"],"metadata":{"id":"LacBHwymVxCq"}},{"cell_type":"markdown","source":["34. What is dimension reduction in machine learning?"],"metadata":{"id":"TP95A6_CVw7j"}},{"cell_type":"markdown","source":["~ Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."],"metadata":{"id":"PQYfzSXVVwyx"}},{"cell_type":"markdown","source":["35. Explain the difference between feature selection and feature extraction."],"metadata":{"id":"uNO_JhQWVwtt"}},{"cell_type":"markdown","source":["~ Feature selection is the process in which we select the feature based on how highly they are co-related with each other and output feature and select any features from them by excluding others with same qualities or unrequired qualities whereas feature extraction is the process of combining various features into one or few features while retaining max possible information of all the features."],"metadata":{"id":"EK-DV4w8VwoV"}},{"cell_type":"markdown","source":["36. How does Principal Component Analysis (PCA) work for dimension reduction?"],"metadata":{"id":"Reqyt9wnVwiu"}},{"cell_type":"markdown","source":["~ PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."],"metadata":{"id":"aanY_3xcVwc7"}},{"cell_type":"markdown","source":["37. How do you choose the number of components in PCA?"],"metadata":{"id":"Z0osXqZZVwWM"}},{"cell_type":"markdown","source":["~ If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."],"metadata":{"id":"uMfN5HLJlCzR"}},{"cell_type":"markdown","source":["38. What are some other dimension reduction techniques besides PCA?"],"metadata":{"id":"8_rlUiqzVv-K"}},{"cell_type":"markdown","source":["~ Low Variance Filter, Missing Values Ratio, Generalized discriminant analysis (GDA), Linear discriminant analysis (LDA), Non-negative matrix factorization (NMF), High Correlation Filter are some other dimension reduction techniques besides PCA."],"metadata":{"id":"8FQ0XgdFVvxS"}},{"cell_type":"markdown","source":["39. Give an example scenario where dimension reduction can be applied."],"metadata":{"id":"vo-gSSLdVva-"}},{"cell_type":"markdown","source":["~ A Cancer dataset has 30 or more features, hence the dimension of the dataset is 30. So, here dimension reduction can be applied to reduce the number of features by feature extraction and covert this high-dimensional data into 2 or 3-dimensional data that can be plotted in a 2D or 3D plot."],"metadata":{"id":"x3rnphA4VvqB"}},{"cell_type":"markdown","source":["**Feature Selection**"],"metadata":{"id":"w4RHcwHhVvih"}},{"cell_type":"markdown","source":["40. What is feature selection in machine learning?"],"metadata":{"id":"IkQjXQqAVufx"}},{"cell_type":"markdown","source":["~ Feature selection, one of the main components of feature engineering, is the process of selecting the most important features to input in machine learning algorithms. Feature selection techniques are employed to reduce the number of input variables by eliminating redundant or irrelevant features and narrowing down the set of features to those most relevant to the machine learning model."],"metadata":{"id":"XaVe6-vbVtyX"}},{"cell_type":"markdown","source":["41. Explain the difference between filter, wrapper, and embedded methods of feature selection."],"metadata":{"id":"vyfrVpu9phSV"}},{"cell_type":"markdown","source":["~ Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In an embedded method, feature selection is integrated or built into the classifier algorithm."],"metadata":{"id":"aKYbjA49phv4"}},{"cell_type":"markdown","source":["42. How does correlation-based feature selection work?"],"metadata":{"id":"2V-WYqQQph-Y"}},{"cell_type":"markdown","source":["\n","~ Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features."],"metadata":{"id":"OV_v0mnWpiK9"}},{"cell_type":"markdown","source":["43. How do you handle multicollinearity in feature selection?"],"metadata":{"id":"wCJd3n0jpiW6"}},{"cell_type":"markdown","source":["~ Following are ways to handle multicollinearity in feature selection :-\n","\n","• A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation.\n","\n","• Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA).\n","\n","• Centering the variables by calculating the mean of every single independent variable and replacing each variable value with the difference between the values and the mean."],"metadata":{"id":"YsqNuT1Opiig"}},{"cell_type":"markdown","source":["44. What are some common feature selection metrics?"],"metadata":{"id":"xf2d-iM6pi4i"}},{"cell_type":"markdown","source":["~ Chi-square Test,\n","Fisher's Score,\n","Correlation Coefficient,\n","Dispersion Ratio,\n","Backward Feature Elimination,\n","Recursive Feature Elimination,\n","Random Forest Importance are some common feature selection metrics."],"metadata":{"id":"DCTZmfEBpjEM"}},{"cell_type":"markdown","source":["45. Give an example scenario where feature selection can be applied."],"metadata":{"id":"4DJyMHNcpjOo"}},{"cell_type":"markdown","source":["~ Selecting the best features helps the model to perform well. For example, Suppose we want to create a model that automatically decides which car should be crushed for a spare part, and to do this, we have a dataset. This dataset contains a Model of the car, Year, Owner's name, Miles. So, in this dataset, the name of the owner, car id, car number etc. kind of columns does not contribute to the model performance as it does not decide if the car should be crushed or not, so we can remove this columns and select the rest of the features(columns) for the model building."],"metadata":{"id":"oKJMlCP6pjX8"}},{"cell_type":"markdown","source":["**Data Drift Detection**"],"metadata":{"id":"ggd0nGZ8pjhK"}},{"cell_type":"markdown","source":["46. What is data drift in machine learning?"],"metadata":{"id":"CZ7nJRqspjqi"}},{"cell_type":"markdown","source":["~ The concept of Data drift in machine learning is an evolution of data that invalidates the data model. It happens when the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes."],"metadata":{"id":"mBiKrSXFpkIY"}},{"cell_type":"markdown","source":["47. Why is data drift detection important?"],"metadata":{"id":"KkSI5f3xpkRm"}},{"cell_type":"markdown","source":["~ Data drift is one of the top reasons model accuracy degrades over time. Data drift can pose significant challenges to machine learning systems in production. So, by understanding the causes and effects of data drift, and implementing effective drift monitoring practices, you can ensure that your machine learning models remain accurate and reliable over time."],"metadata":{"id":"G-wgpIIrpkbL"}},{"cell_type":"markdown","source":["48. Explain the difference between concept drift and feature drift."],"metadata":{"id":"a9dlW2uDpkom"}},{"cell_type":"markdown","source":["~ Feature drift refers to the changing distribution of the data to which the model is applied whereas Concept drift refers to a changing underlying goal or objective for the model."],"metadata":{"id":"O96KSpLGpkyi"}},{"cell_type":"markdown","source":["49. What are some techniques used for detecting data drift?"],"metadata":{"id":"tzRkkBdlpmIH"}},{"cell_type":"markdown","source":["~ Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric are some techniques used for detecting data drift."],"metadata":{"id":"m4zI9o9ipmck"}},{"cell_type":"markdown","source":["50. How can you handle data drift in a machine learning model?"],"metadata":{"id":"Xf-W83bapmmE"}},{"cell_type":"markdown","source":["\n","~ Steps to handle data drift in ML model :-\n","\n","1. Check the data quality\n","2. Investigate the drift\n","3. Don't build model further if data drift is found  \n","4. Retrain it, if you can\n","5. Calibrate or rebuild the model\n","6. Pause the model and use a fallback\n","7. Find the low-performing segments\n","8. Apply business logic on top of the model\n","Data drift"],"metadata":{"id":"qCle5vJDpmvf"}},{"cell_type":"markdown","source":["**Data Leakage**"],"metadata":{"id":"WtDOLlZepm5D"}},{"cell_type":"markdown","source":["51. What is data leakage in machine learning?"],"metadata":{"id":"d6iCRgaFpnDf"}},{"cell_type":"markdown","source":["~ Data leakage happens when the training data contains information about the target, but similar data won't be available when the model is used for prediction. This leads to high performance on the training dataset (and possibly even the validation data), but the model will perform badly in production."],"metadata":{"id":"Ngq9qVLapnN1"}},{"cell_type":"markdown","source":["52. Why is data leakage a concern?"],"metadata":{"id":"pIvLI4MbpnYC"}},{"cell_type":"markdown","source":["~ Data leakage is one of the main machine Learning errors and can affect the overall production performance and validation accuracy of the model to a great extent. Data leakage must be resolved to obtain a robust and generalized predictive model."],"metadata":{"id":"DJy5xJaVpnsd"}},{"cell_type":"markdown","source":["53. Explain the difference between target leakage and train-test contamination."],"metadata":{"id":"Qiek-dmmpn26"}},{"cell_type":"markdown","source":["~ Target leakage can happen when a variable that is not a feature is being used to predict the target whereas train-test contamination happens when we unknowingly or subtly pass information from our train dataset to our validation dataset."],"metadata":{"id":"fJvbVno1poBA"}},{"cell_type":"markdown","source":["54. How can you identify and prevent data leakage in a machine learning pipeline?"],"metadata":{"id":"L78Ok5sapoLL"}},{"cell_type":"markdown","source":["~ If the test data is used along with training data when using fit or fit-tranform methods, data leakage will occur. To prevent such leakage, the dataset should be separated into training and testing sets before performing any preprocessing steps. The preprocessing steps should only be fit on the training dataset and then applied to both the training and test datasets separately."],"metadata":{"id":"DgaPICAUpoVD"}},{"cell_type":"markdown","source":["55. What are some common sources of data leakage?"],"metadata":{"id":"Ta10vg5JpohZ"}},{"cell_type":"markdown","source":["~ Data leakage generally occurs when the training data is overlapped with testing data during the development process of ML models by sharing information between both data sets. Ideally, there should not be any interaction between these data sets, training and test sets. Poor data security and sanitization, outdated systems, or a lack of employee training are also some common sources of data leakage."],"metadata":{"id":"ga1QT030pov6"}},{"cell_type":"markdown","source":["56. Give an example scenario where data leakage can occur."],"metadata":{"id":"zXjfmG6GppCm"}},{"cell_type":"markdown","source":["~ Unsafe servers :\n","It is not only important to send data securely, it should also be stored securely.\n","As early as January 2022 has an unsecured server resulted in the exposure of 3 terabyte of data including airport employee records. Among the data were ID card photos, personally identifiable information and national ID numbers. That's exactly the information cyber criminals need for identity fraud. So by using secure servers, you can prevent data from being viewed just like that. It is important that these servers are not only secured themselves, but that they also store the data encrypted."],"metadata":{"id":"PeGZ3XAYppWj"}},{"cell_type":"markdown","source":["**Cross Validation**"],"metadata":{"id":"AxngrorEppgs"}},{"cell_type":"markdown","source":["57. What is cross-validation in machine learning?"],"metadata":{"id":"fGUcNuMNpppz"}},{"cell_type":"markdown","source":["~ Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds. This process is repeated multiple times, each time using a different fold as the validation set. Finally, the results from each validation step are averaged to produce a more robust estimate of the model’s performance."],"metadata":{"id":"Z6jMPf6Upnin"}},{"cell_type":"markdown","source":["58. Why is cross-validation important?"],"metadata":{"id":"NCVSi6w5pmTC"}},{"cell_type":"markdown","source":["~ The main importance of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model's generalizability. It also helps to avoid overfitting, which is a common problem in machine learning."],"metadata":{"id":"IEj7wZ58VaNm"}},{"cell_type":"markdown","source":["59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation."],"metadata":{"id":"UhvBP1p4plyJ"}},{"cell_type":"markdown","source":["~ K-Fold cross-validation divides the dataset into k folds whereas Stratified k-fold cross-validation ensures that each fold of dataset has the same proportion of observations with a given label. K-fold cross-validation uses Random Sampling whereas Stratified k-fold cross-validation uses Stratified Sampling"],"metadata":{"id":"W4nFii8fpj9C"}},{"cell_type":"markdown","source":["60. How do you interpret the cross-validation results?"],"metadata":{"id":"AUjStZ1ZpiuT"}},{"cell_type":"markdown","source":["~ Cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance. In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance."],"metadata":{"id":"q09ApFhapj0N"}}]}